{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Word Embedding\n",
    "In NLP, word embedding is a term used for the representation of words for text analysis, typically in the form of a real-valued vector that encodes the meaning of the word such that the words that are closer in the vector space are expected to be similar in meaning.\n",
    "\n",
    "###### Word Embedding are of two types\n",
    "1. Count of Frequency\n",
    "   1. One Hot Encoding\n",
    "   2. Bag of Words\n",
    "   3. TF-IDF\n",
    "2. Deep Trained Model\n",
    "   1. Word to Vec\n",
    "      1. CBOW(Continious Bag of words)\n",
    "      2. Skipgram\n",
    "   \n",
    "\n",
    "**Word To Vec:** It is a technique for natural language processing published in 2013. The Word2Vec algorithm uses a neural network model to learn word associations from a large corpus of text. Once trained, such a model can detect synonymous words or suggest additional words for a partial sentence. As the name implies, word2vec represents each distinct word with  a particular list of numbers called a vector.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
